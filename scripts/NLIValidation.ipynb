{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjQODWsO5woI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-xsmall')\n",
        "\n",
        "def process_json(input_file, output_file):\n",
        "    # Load the JSON data\n",
        "    with open(input_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Define the label mapping\n",
        "    label_mapping = ['contradiction', 'entailment', 'neutral']\n",
        "\n",
        "    # Initialize a counter for the items\n",
        "    item_number = 0\n",
        "\n",
        "    # Process each item in the JSON\n",
        "    for item in data:\n",
        "        # Increment the item counter\n",
        "        item_number += 1\n",
        "\n",
        "        # Extract the premise and hypothesis from the JSON item\n",
        "        premise = item['Passage']\n",
        "        hypothesis = item['Question']\n",
        "\n",
        "        # Tokenize the premise and hypothesis\n",
        "        features = tokenizer(premise, hypothesis, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Perform inference using the pre-trained model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(**features).logits\n",
        "            prediction = label_mapping[logits.argmax(dim=1).item()]\n",
        "\n",
        "        # Add the 'Relation' key with the model's prediction\n",
        "        item['Relation'] = prediction\n",
        "\n",
        "        # Print the item number for progress tracking\n",
        "        print(f\"Item number: {item_number}\")\n",
        "\n",
        "    # Save the modified data to a new JSON file\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "# Replace the following with your specific filename and folder paths\n",
        "filename = \"YourDocumentName\"\n",
        "\n",
        "# Specify the input and output file paths\n",
        "input_file = f'RawQuestions/{filename}_questions.json'\n",
        "output_file = f'SilverQuestions/{filename}_QA_NLI.json'\n",
        "\n",
        "# Process the JSON file\n",
        "process_json(input_file, output_file)\n"
      ]
    }
  ]
}